
# these are set in the Makefile
root: ""
cuda: ""
deps: false
gpus: ""
workspace: ""

experiment:
  name: snakemake
  src: ru
  trg: en

  teacher-ensemble: 2
  # path to a pretrained backward model (optional)
  backward-model: ""

  # limits per downloaded dataset
  mono-max-sentences-src: 100000
  mono-max-sentences-trg: 200000

  # split corpus to parallelize translation
  split-length: 100000

  best-model: chrf

  bicleaner:
    default-threshold: 0.5
    dataset-thresholds:
      mtdata_neulab_tedtalksv1_train: 0.6

training:
  after-epochs: 1


datasets:
  # parallel corpus
  train:
    - opus_ada83/v1
    - opus_GNOME/v1
    - mtdata_neulab_tedtalksv1_train
  devtest:
    - flores_dev
    - mtdata_newstest2019_ruen
  test:
    - flores_devtest
    - sacrebleu_wmt20
    - sacrebleu_wmt18
  # monolingual datasets (ex. paracrawl-mono_paracrawl8, commoncrawl_wmt16, news-crawl_news.2020)
  # to be translated by the teacher model
  mono-src:
    - news-crawl_news.2020
  # to be translated by the shallow backward model to augment teacher corpus with back-translations
  # leave empty to skip augmentation step (high resource languages)
  mono-trg:
    - news-crawl_news.2020


