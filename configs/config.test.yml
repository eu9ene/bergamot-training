dirs:
  data-root: <shared-root>/bergamot
  cuda: <cuda-dir>

experiment:
  name: snakemake
  src: ru
  trg: en

  teacher-ensemble: 2
  # path to a pretrained backward model (optional)
  backward-model: ""

  # limits per downloaded dataset
  mono-max-sentences-src: 100000
  mono-max-sentences-trg: 200000

  bicleaner-threshold: 0.5

resources:
  gpus: <gpus>
  workspace: <workspace>
  # split corpus to parallelize translation
  split-length: 100000

datasets:
  # parallel corpus
  train:
    - opus_ada83/v1
    - opus_GNOME/v1
    - mtdata_JW300
  devtest:
    - flores_dev
    - mtdata_newstest2019_ruen
  test:
    - flores_devtest
    - sacrebleu_wmt20
    - sacrebleu_wmt18
  # monolingual datasets (ex. paracrawl-mono_paracrawl8, commoncrawl_wmt16, news-crawl_news.2020)
  # to be translated by the teacher model
  mono-src:
    - news-crawl_news.2020
  # to be translated by the shallow backward model to augment teacher corpus with back-translations
  # leave empty to skip augmentation step (high resource languages)
  mono-trg:
    - news-crawl_news.2020


