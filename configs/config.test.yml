
root: ""
cuda: ""
deps: false
gpus: ""
workspace: ""

experiment:
  name: snakemake
  src: ru
  trg: en

  teacher-ensemble: 2
  # path to a pretrained backward model (optional)
  backward-model: ""

  # limits per downloaded dataset
  mono-max-sentences-src: 100000
  mono-max-sentences-trg: 200000

  bicleaner-threshold: 0.5

  split-length: 100000

training:
  after-epochs: 1


datasets:
  # parallel corpus
  train:
    - opus_ada83/v1
    - opus_GNOME/v1
    - mtdata_JW300
  devtest:
    - flores_dev
    - mtdata_newstest2019_ruen
  test:
    - flores_devtest
    - sacrebleu_wmt20
    - sacrebleu_wmt18
  # monolingual datasets (ex. paracrawl-mono_paracrawl8, commoncrawl_wmt16, news-crawl_news.2020)
  # to be translated by the teacher model
  mono-src:
    - news-crawl_news.2020
  # to be translated by the shallow backward model to augment teacher corpus with back-translations
  # leave empty to skip augmentation step (high resource languages)
  mono-trg:
    - news-crawl_news.2020


